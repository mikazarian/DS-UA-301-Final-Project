# DS-UA-301-Final-Project

The main limitation of large language models we seek to address is hallucination, in particu-
lar regarding evaluation of misinformative political claims. A promising direction to address
this limitation is to augment LLMs with external tools—such as knowledge APIs—that al-
low them to retrieve and incorporate verified evidence during reasoning. This project uses
a logical framework based on scores for corroboration, bias-checking, and evidence strengh
to guide llm reasoning and mitigate possible hallucinations by using article retrieva
