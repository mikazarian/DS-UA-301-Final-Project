# ðŸ§  Reducing LLM Hallucination in Political Fact-Checking with Tool Augmentation and Custom Scoring

**Michael Kazarian (NYU)**  
DS-UA 301 â€” Final Project

---

## ðŸ“Œ Overview

Large Language Models (LLMs) often **hallucinate** when evaluating political claims, especially when those claims require historical or contextual knowledge. This project investigates whether **tool augmentation** (e.g., Wikipedia retrieval) and **structured evidence scoring** can reduce hallucinations and improve factual classification.

We compare three systems:

1. **Baseline LLM** â€” direct prompt, no tools
2. **Tool-Augmented ReACT Agent** â€” Wikipedia retrieval via LangChain
3. **Custom-Prompted Evidence Scoring System** â€” SERP API retrieval + GPT-4 scoring with quantitative trust metrics

We evaluate performance on the **LIAR political fact-checking dataset** and analyze why some approaches surprisingly underperform.

---

## ðŸŽ¯ Key Insight

> Simply giving an LLM access to tools **does not** guarantee better factual reasoning.

| Model | Accuracy | Weighted F1 |
|-------|----------|-------------|
| Baseline (no tools) | **47.3%** | 0.408 |
| Tool-Augmented (ReACT + Wikipedia) | **43.3%** | 0.333 |
| **Custom Evidence Scoring (SERP + GPT-4)** | **69%** | **0.73** |

The best system was **not** the one with the most tools, but the one that **forced the LLM into a structured quantitative reasoning framework**.

---

## ðŸ§ª Dataset

We use the **LIAR dataset** (12,836 political claims).

- Original labels: 6 truth categories
- Binarized into: **True / False**
- Evaluation set: first **300 claims** from official test split
- Metrics: Accuracy, F1, Confusion Matrices

---

## âš™ï¸ Methodology

### 1ï¸âƒ£ Baseline Prompting (GPT-3.5)

The model receives only the claim and must answer:

```
Answer: True/False
Justification: ...
```

No external tools.

---

### 2ï¸âƒ£ Tool-Augmented ReACT Agent (LangChain + Wikipedia)

- Agent performs up to 5 reasoning steps
- Uses Wikipedia API tool for evidence retrieval
- Integrates evidence before final decision

**Problem observed:**  
Agent over-relied on retrieval. If it failed to find relevant evidence, it defaulted to incorrect conclusionsâ€”even when the LLM â€œknewâ€ the answer from training.

---

### 3ï¸âƒ£ Custom Evidence Scoring System (SERP API + GPT-4)

This system mimics **human fact-checkers**.

For each claim, we retrieve 10 articles and compute:

#### âœ” Credibility Score
\[
\frac{N_{corroborating}}{N_{corroborating} + N_{contradicting}}
\]

#### âœ” Bias Score
Political leaning + emotional tone

#### âœ” Evidence Strength
\[
\frac{N_{strong}}{N_{strong} + N_{weak}}
\]

#### âœ” Trust Score
\[
w_1 \cdot credibility + w_2 \cdot (1 - |bias|) + w_3 \cdot evidence
\]

If trust score > 0.5 â†’ **True**

The LLM outputs structured JSON, removing open-ended reasoning.

---

## ðŸ“Š Results

- Both baseline and tool models had **very poor recall on False claims**
- Tool model performed worse because of **retrieval failure**
- Custom scoring system significantly improved balance

(See report for confusion matrices and examples.)

---

## ðŸ” Qualitative Findings

Example failure case:

> **Claim:** Obamaâ€™s NLRB sued Boeing over opening a plant in SC  
> **Ground Truth:** True  
> **Baseline:** True  
> **Tool Agent:** False  

The tool agent couldnâ€™t retrieve the historical context, while the baseline LLM remembered it from training.

---

## ðŸ§  Why Custom Prompting Worked

The custom system:

- Reduced reliance on open-ended reasoning
- Leveraged LLM strengths: sentiment, semantic similarity, classification
- Forced **quantitative evidence aggregation**
- Minimized hallucination by avoiding narrative justification

---

## ðŸ› ï¸ Tech Stack

- Python
- LangChain (ReACT agent)
- Wikipedia API
- SERP API
- GPT-3.5 / GPT-4
- sklearn for evaluation

---

## ðŸ“ Project Structure

```
/data
/baseline
/react_agent
/custom_scoring
/evaluation
/prompts
report.pdf
```

---

## ðŸš§ Limitations

- LIAR dataset claims are not time-stamped
- Wikipedia retrieval is keyword-based and often irrelevant
- Binary labels oversimplify nuanced claims
- SERP API limited to 10 articles

---

## ðŸ”® Future Work

- Multi-class truth labels (partially true, etc.)
- ROC-based trust threshold tuning
- Improved retrieval beyond keyword search
- Tool usage that supports, not replaces, LLM reasoning

---

## ðŸ Takeaway

> The key to reducing LLM hallucination was **not** giving the model more information â€”  
> it was **forcing the model to reason in a structured, quantitative way**.

This project demonstrates how prompt design and evidence scoring can outperform naive tool augmentation in factual reasoning tasks.
