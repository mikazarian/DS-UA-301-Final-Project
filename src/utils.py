# -*- coding: utf-8 -*-
"""utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BSZ0YGtq2dGW9ysM0jQ9ruBpEs9gCh1B

Utility functions and classes for LLM-based fact-checking with tool-augmented reasoning.

This module provides:
- ReACT agent for evidence-based fact-checking
- Custom LangChain tools for evidence retrieval
- Helper functions for prompt handling and result processing
"""

import pandas as pd
import json
import re
import os
from typing import Dict, List, Optional, Any
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.agents import Tool, AgentExecutor, create_react_agent
from langchain.tools import BaseTool
from langchain.callbacks.manager import CallbackManagerForToolRun
from langchain import hub
import logging

from google.colab import drive
drive.mount('/content/drive')

os.chdir('/content/drive/MyDrive/DS301_Final_Project')
from tools.wikipedia_tool import WikipediaTool
from tools.newsapi_tool import NewsAPITool

from dotenv import load_dotenv
env_path = "/content/drive/MyDrive/env.txt"
load_dotenv(dotenv_path=env_path)


logger = logging.getLogger(__name__)

_news_failed_claims = {}

class EvidenceRetrievalTool(BaseTool):
    """
    Custom LangChain tool for retrieving evidence from multiple sources.

    This tool integrates Wikipedia and news APIs to gather relevant evidence
    for fact-checking claims. It formats the evidence in a structured way
    that the ReACT agent can understand and reason with.
    """

    # Required Pydantic annotations for LangChain BaseTool
    name: str = "evidence_retrieval"
    description: str = "Search for evidence about a factual claim using Wikipedia and news sources"
    
    
    
    

    def _run(self, claim: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        global _news_failed_claims
        """
        Search for evidence about the given claim.
        
        This method:
        1. Searches Wikipedia for relevant information
        2. Optionally searches news sources if news_tool is available
        3. Combines and formats all evidence for the agent
        
        Args:
        claim: The factual claim to search evidence for
        run_manager: LangChain callback manager (unused but required)
        
        Returns:
        Formatted string containing numbered evidence pieces
        """
        evidence_pieces=[]
        
    
        # SCRATCH THIS - NOT USING NEWS! ONLY WIKIPEDIA
        #if claim not in _news_failed_claims:
        #try:
            #news_api_key=os.getenv("NEWS_API_KEY")
            #print(f"\nDEBUG: news_api_key value inside _run: {news_api_key}")
            #print(f"\nDEBUG: news_api_key type: {type(news_api_key)}")
            #if news_api_key and "TRIED_NEWS" not in claim:
                #news_tool = NewsAPITool(api_key=news_api_key, max_results=3, max_chars=400)
                #news_evidence = news_tool.search_evidence(claim)
                #if len(news_evidence) == 0:
                   #_news_failed_claims[claim] = True
                 #  claim += " TRIED_NEWS"
                #else:
                 #   logger.info(f'\nDEBUG: News returned {len(news_evidence)} results')
                  #  evidence_pieces.extend(news_evidence)
            #else:
             #  print("DEBUG: No news API key available")
        #except Exception as e:
         #   _news_failed_claims[claim] = True
          #  print(f"DEBUG: News tool error: {e}")
        
        try:
            wikipedia_tool = WikipediaTool(max_results=3, max_chars=400)
            evidence_pieces = wikipedia_tool.search_evidence(claim)
        except:
            evidence_pieces = []
        
        if not evidence_pieces:
            return '\nNo evidence found for this claim.\n'
        
        # Format evidence as numbered list for the agent to process
        formatted = "\n\nEvidence found:"
        for i, evidence in enumerate(evidence_pieces, 1):
          formatted += f"\n{i}. {evidence['source']}: {evidence['content']}"

        return formatted.strip()

class FactCheckingAgent:
    """
    ReACT-based agent for fact-checking claims with evidence retrieval.

    This agent implements the ReACT (Reasoning + Acting) pattern:
    1. Receives a claim to fact-check
    2. Reasons about what evidence is needed
    3. Acts by using the evidence retrieval tool
    4. Reasons about the evidence to make a judgment
    5. Provides a final True/False answer with justification
    """

    def __init__(self, wikipedia_tool, news_tool=None, llm_model: str = "gpt-3.5-turbo"):
        """
        Initialize the fact-checking agent.

        Args:
            wikipedia_tool: Tool for searching Wikipedia
            news_tool: Optional tool for searching news
            llm_model: Name of the LLM model to use (default: gpt-3.5-turbo)
        """
        # Initialize the language model with low temperature for consistent reasoning
        self.llm = ChatOpenAI(model_name=llm_model, temperature=0.2) #!

        # Create the evidence retrieval tool that the agent will use
        self.evidence_tool = EvidenceRetrievalTool()
        self.tools = [self.evidence_tool]

        # Load the ReACT prompt template from LangChain hub
        # This prompt teaches the agent how to reason and act iteratively
        self.prompt = hub.pull("hwchase17/react")

        # Create the ReACT agent and executor
        self.agent = create_react_agent(self.llm, self.tools, self.prompt)
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True, # Show reasoning steps for debugging
            max_iterations=7, # Limit iterations to prevent infinite loops
            early_stopping_method="force"
        )

    def fact_check_with_reasoning(self, claim: str) -> Dict[str, str]:
        """
        Fact-check a claim using ReACT reasoning with evidence retrieval.

        The agent will:
        1. Analyze the claim
        2. Decide what evidence to search for
        3. Use the evidence_retrieval tool
        4. Reason about the evidence
        5. Provide a True/False judgment with justification

        Args:
            claim: The factual claim to verify

        Returns:
            Dictionary with 'prediction' (True/False) and 'justification' (reasoning)
        """

        # Construct the prompt that guides the agent's reasoning process
        prompt = f"""
        Analyze this claim and determine if it is TRUE or FALSE.
        Only answer TRUE or FALSE; no other answer than TRUE or FALSE is allowed!
        (Even when the evidence may be "inconclusive," still only answer TRUE or FALSE.)
        If you cannot find sufficient evidence after searching, make your best judgment based on available information; 
        you still MUST answer either TRUE or FALSE. Moreover, if you reach the end of the fifth reasoning iteration, you MUST
        make a decision at that point.

        Claim: "{claim}"

        Use the evidence_retrieval tool to gather information, then provide your analysis.

        You MUST format your final response as:
        Answer: [True/False]
        Justification: [Your reasoning based on the evidence]
        
        NOTE that the justification is also mandatory.
        """

        try:
            # Let the agent reason through the problem using ReACT pattern
            response = self.agent_executor.invoke({"input": prompt})
            # Parse the agent's final output to extract prediction and justification
            return parse_llm_response(response.get("output", ""))
        except Exception as e:
            # Handle any errors during agent execution
            logger.error(f"Agent error: {e}")
            return {"prediction": "Error", "justification": f"Failed to process: {str(e)}"}

def simple_llm_call(prompt: str, system_message: str = None, model: str = "gpt-3.5-turbo") -> str:
    """
    Make a simple call to the language model without tool usage.

    This function is used for baseline fact-checking where the LLM
    relies only on its internal knowledge without external evidence.

    Args:
        prompt: The user prompt/question
        system_message: Optional system message to guide behavior
        model: LLM model name to use

    Returns:
        Raw text response from the LLM
    """
    llm = ChatOpenAI(model_name=model, temperature=0.1)

    # Construct message list with optional system message
    messages = []
    if system_message:
        messages.append(SystemMessage(content=system_message))
    messages.append(HumanMessage(content=prompt))

    try:
        response = llm(messages)
        return response.content.strip()
    except Exception as e:
        logger.error(f"LLM error: {e}")
        return "Error: Could not get response"

def parse_llm_response(response: str) -> Dict[str, str]:
    """
    Parse LLM response to extract structured prediction and justification.

    Looks for specific patterns in the response:
    - "Answer: True" or "Answer: False" for the prediction
    - "Justification: ..." for the reasoning

    Args:
        response: Raw text response from LLM or agent

    Returns:
        Dictionary with 'prediction' and 'justification' keys
    """
    result = {"prediction": "Unknown", "justification": ""} # Initials
    
    print(f"DEBUG: Parsing response: {repr(response[:200])}")
    # Extract the True/False answer using regex
    answer_match = re.search(r'(?:Answer:\s*)?(True|False)', response, re.IGNORECASE)
    if answer_match:
        result["prediction"] = answer_match.group(1).capitalize()
        print(f"DEBUG: Found prediction: {result['prediction']}\n")
    else:
        print("DEBUG: No True/False found in response")

    # Extract the justification text
    justification_match = re.search(r'Justification:\s*(.+)', response, re.DOTALL | re.IGNORECASE)
    if justification_match:
        result["justification"] = justification_match.group(1).strip()
    else:
        # Fallback: use entire response if no clear justification found
        result["justification"] = response

    return result

def load_prompt(prompt_file: str) -> str:
    """
    Load a prompt template from the prompts directory.

    Args:
        prompt_file: Filename of the prompt template (e.g., 'baseline_prompt.txt')

    Returns:
        Content of the prompt file as a string
    """
    with open(f"prompts/{prompt_file}", 'r') as f:
        return f.read().strip()

def save_results(results: List[Dict], output_file: str):
    """
    Save experimental results to a JSONL file.

    Each result dictionary is saved as one line of JSON,
    making it easy to process large result sets incrementally.

    Args:
        results: List of result dictionaries to save
        output_file: Path where to save the results
    """
    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(output_file), exist_ok=True)

    # Write each result as a separate JSON line
    with open(output_file, 'w') as f:
        for result in results:
            f.write(json.dumps(result) + '\n')
    #print(f"\nResults saved to {output_file}")

def load_results(results_file: str) -> List[Dict]:
    """
    Load experimental results from a JSONL file.

    Args:
        results_file: Path to the results file

    Returns:
        List of result dictionaries
    """
    results = []
    with open(results_file, 'r') as f:
        for line in f:
            results.append(json.loads(line.strip()))
    return results

